% !Mode:: "TeX:UTF-8"

\chapter{轨迹挖掘理论基础}
\label{chapter:rw}

随着轨迹数据的大量产生，获取轨迹数据难度的降低，对轨迹数据的挖掘和表征得到了越来越多科研工作者的注意。挖掘轨迹中的语义信息对分析人类、车辆、动物和气候变化等行为有重要的意义，对基于位置的应用也提供了很多帮助。通常我们获取到的原始轨迹数据都是不包含太多语义信息的时空数据，这些数据通常数目巨大，格式、内容不同，长短不一，因此在挖掘轨迹数据之前寻求合适的压缩方法及合理的轨迹表征方法就显得非常重要。本章节将首先介绍目前轨迹数据挖掘中轨迹表征领域的相关工作，再针对性的对已有方法的不足做出总结。之后，本章节将探讨地点推荐的常用科技以及现状，一些重要的问题将在本章中被揭示，为后面本文的创新点做出铺垫。



\section{轨迹压缩方法小结}
根据Renso等人\citeup{Renso:2013:MDM:2553228}关于轨迹压缩的全面的综述，轨迹压缩的目标大体可以分为三个：（1）减少轨迹数据集的规模，从而使得海量数据达到方便处理的程度； （2）提升轨迹数据集上的计算效率；（3）确保压缩后的轨迹对比压缩前的轨迹没有太大的偏差，或者偏差在一个可接受的阈值内。下面是一些主流的轨迹压缩方法，其大体分为两类：单纯轨迹压缩方法与考虑语义的轨迹压缩方法。下面将展开介绍：

\subsection{单纯轨迹压缩}
为了解决轨迹数据增长迅速的问题，很多研究者提出了各种轨迹压缩的科技。Sun等人\citeup{sun2016overview}给了一个轨迹压缩的综述。其中，早期的工作多是把轨迹视为一些直线的衔接组合，这就把轨迹压缩变为了欧式空间的线条的简化问题。 例如，早期最出名的Douglas-Peucker算法\citeup{douglas1973algorithms}的思想是，迭代的选出轨迹中间的一些点，使得这些点到去除这个点的线段的垂线距离不超过一个给定阈值范围，这个过程将不断进行，直到剩下的所有点都满足条件。如图\ref{DP}所示，链接$A,F$两点，在一定阈值范围里（红局限的短边长），$D$点是第一个超过阈值的的点，于是把轨迹划分为$AD$与$DF$两段，继续找到阈值外的$C$点。其余点都不超出阈值，于是在简化中被去除。最终，压缩后的轨迹如图\ref{DP}(d)所示。

\pic[!htb]{Douglas-Peucker算法示意图}{width=150mm}{DP}

在Douglas-Peucker算法的基础上，一些算法\citeup{meratnia2004spatiotemporal,potamias2006sampling}将原始的距离度量改为了同步的欧式距离（Synchronized Euclidean Distance，SED），这样轨迹的时间信息也被考虑了进去了。例如Meratnia等人\citeup{meratnia2004spatiotemporal}就在SED的度量上提出了一个从顶向下的时间率（TD-TR）和一个打开窗口的时间率算法。在算法的开始，从第一到第三个采样点被一根线段练了起来，如果SED度量从中间第二个点到这个线段投影的距离随着时间流逝始终小于某一个阈值的话，则算法向前移动一个采样点继续运行；否则，若这个距离超过一个阈值的话，这个使得阈值超过的点成为原起点段的终点，和后一段的起点。

此外，Potamias等人\citeup{potamias2006sampling}提出了两种算法，分别是Thresholds和STTrace，来对在线的轨迹流进行处理。这两个算法利用了GPS坐标、速度、目前位置的方向来计算出一个安全的区域，以估计下一时刻的所在区域。如果下个位置确实来到了估算中的安全区域，那算法将继续，否则将处理异常来对现有参数进行矫正。Lee等人\citeup{lee2007trajectory}以及Soares等人\citeup{soares2015grasp}用最小描述长度（MDL）来对轨迹压缩的程度进行控制，其基本原理是找寻一个准确度与压缩率的折中。Muckell等人提出了空间质量简化的启发方法（SQUISH）,其基本的原理是将优先度付给那些轨迹流中重要的采样点，将冗余的采样点永久地去除。之后他们又改进了这一方法，取名为了SQUISH-E,让调节压缩率和错误率更为方便\citeup{muckell2014compression}。还有一些方法提出了处理在线轨迹的压缩模式。例如，Dead Reckoning算法用现在目标的位置和速度来估计之后目标的位置信息\citeup{trajcevski2006line}。而Liu等人\citeup{liu2015bounded}提出的BQS算法通过计算新点与一个在维护的线段的距离来进行保留点选择。而Lin等人\citeup{lin2017one}提出了一种只过一遍的策略来计算轨迹中每个点的误差以确定压缩轨迹的点保留情况。

值得一提的是，这些在线算法仅仅是输出一个压缩后的轨迹集合，这任然没有解决原始轨迹杂乱无章的格式的问题，含时间的检索仍然成问题。更重要的是，这些方法都是将轨迹视为独立的线条组合来考虑的，并没有考虑城市中复杂的交通情况、丰富的重要节点。所以这些方式只能视为是轨迹降采样的方法，并不适合用来建立全局的城市轨迹档案。


\subsection{语义轨迹压缩}
如同Parent等人的研究指出那样，在轨迹上的大多数应用研究分析都要求更多的额外信息了。举个例子，如果要分析解释某个用户的行为动机，那么城市中的额外的信息例如交通地图或者重要的POI点就会很重要。建立在这些额外信息的基础上，则原本时空坐标序列可以被代替为街道的组合，或者重要商店、餐厅的序列。这样，原本普通的原始轨迹就被赋予了丰富的语义信息。


\pic[!htb]{语义轨迹压缩（STC）算法示意图\citeup{richter2012semantic}}{width=150mm}{STC}

在这种思路的指引下，很多研究者纷纷加入合适的额外信息。比如Schmid等人\citeup{schmid2009semantic,richter2012semantic} 提出了一种语义轨迹压缩（STC）的方式，其利用城市的道路网，将原始轨迹直接代替成为了这个网络上的节点和边，大大简化了轨迹的表示。STC的基本思路可由图\ref{STC}表示。其中图\ref{STC}(a)是一条原始轨迹序列，(b)中可视化了其在空间中的样貌。在(c)中城市的路网被一起加入可视化，可看到轨迹顺着街道穿梭。在(d)中，这条原始轨迹被用街道衔接组合了起来，成为一个街道的串联。并且每一个节点的用时信息也被保留下来。


这也是一种全局的表示方法，本文采用的方法就是这种思路。然而，STC这个方法也不是没有缺陷，其完全依赖于城市交通网络，也就是说，当这个网络的信息有缺失或者稀疏，则这种表征的性能将大大收到影响。其次，交通网络只能给出汽车的形势轨迹，其对骑行轨迹以及步行轨迹的刻画也不全面。之后，Liu等人\citeup{liu2014compressing}延续这个思路进一步地提出了基于速度和锚点的轨迹简化算法，其目标也是同时将所有轨迹同时压缩简化为固定锚点与速度的表示。此外，还有另外的方法着重考虑轨迹数据中的一些行为模式，例如Chen等人\citeup{chen2009trajectory}以及Zheng等人\citeup{zheng2010understanding}利用了轨迹的速度、加速度还有方向改变快慢的信息，将轨迹切割为了行走段和非行走段，并维护了轨迹的骨架信息以及语义信息。在这之后，他们进一步调节了非行走段的部分，使之被更细节的交通方式所刻画，比如自行车、公交车以及自驾。这个过程用了一系列不同的科技，比如监督学习到决策树推理，再比如一个增强准确度刻画的后续处理流程。

此外，一些研究者开始更加注重轨迹的方向信息，强调着轨迹的方向信息里蕴含保留了更多的轨迹结构信息。比如，Song等人\citeup{song2014press}提出了一个叫做PRESS的框架来将轨迹分为时间部分和空间部分，并分别针对这两部分提出两种两个算法，一个是混合空间压缩（HSC）算法，另一个是误差限制的时间压缩（BTC）算法。

也有研究者开始强调城市路网的重要性，认为轨迹应该投影到路网上。而这个过程中，路网匹配的技术开始被广泛研究和改进\citeup{civilis2005techniques,gotsman2015dilution,popa2015spatio,dong2018novel}，更多的相关信息可以参阅综述\citeup{Renso:2013:MDM:2553228,zheng2015trajectory,mazimpaka2016trajectory}。

值得一提的，现有大多数的方法都是假设城市的交通网络和重要语义节点是全面的，没有考虑这些信息部分缺失或者稀疏的情况，比如路网在某些部分没有收录信息。若是碰到这种情况，则上述的算法将失去作用，其性能将大大收到影响。基于这个动机，本文利用提出了一种全新的轨迹聚类压缩算法，使得算法能很好地工作在无论是有路网还是路网确实的环境中。

\section{语义轨迹表征}

为了从轨迹数据中提取有用的知识，最主要的步骤是生成一个良好的轨迹表示。然而，由于轨迹数据的无序性，例如，不同的采样率和实际应用中不同的轨迹长度，这仍然是一项具有挑战性的任务。

\subsection{轨迹表征方法总结}
目前，根据相关的轨迹挖掘场景，主流的轨迹表示主要分为三种：基于轨迹采样点的表征\citeup{yuan2013t,zheng2011learning,anagnostopoulos2017tour}，基于轨迹线段的表征\citeup{bellman1961approximation,lee2007trajectory,lee2011trajectory}和基于轨迹特征的表示\citeup{annoni2012analysis,ardakani2017encoding,pelekis2017temporal}。

\begin{itemize}
    \item \textbf{~~基于点的表征：}其总体思想是识别轨迹中的关键点，然后使用这些点来表征这条轨迹。关键点包括驻点和停留点等。Zheng等人\citeup{zheng2009mining}在2009年定义在一个给定的时间和空间阈值内的一系列GPS点的中心点是一个驻点，由此将一条由GPS点连成的轨迹划分为一系列的驻点，并根据用户的历史轨迹来衡量用户之间的相似性，接着他们在2011年使用同样的表征方法\citeup{zheng2011learning}，实现了用户的旅游推荐。Alvares等人\citeup{alvares2007model}在2007年提出了新的轨迹划分的方式，将轨迹表示为一系列的停留点和连接这些停留点的“运动”。基于点的轨迹表征很简单直观，但是使用少量点表征一整条轨迹能表达的轨迹语义信息是很有限的，轨迹的时间信息也被忽略，因此难以很好的表征轨迹的时空语义信息。

    \item \textbf{~~基于分段的表征：}其核心思想是将轨迹分割为若干部分，然后使用这些部分来表征这条轨迹。这种方法一般假设每一个轨迹分段在几何形状比较平滑，或者有特殊的语义标注。Chen等人\citeup{chen2009trajectory}在2008年，根据移动物体位置和速度的区别，将轨迹表征为行走（walk）和非行走（non-walk）。Lee等人\citeup{lee2011trajectory}在2011年使用了基于最小描述长度的轨迹分割方法，考虑到轨迹在现实世界中受到道路路径的限制，Song等人\citeup{paefgen2011gps}在2014年提出了一种基于路网匹配的轨迹分段方法，将GPS轨迹映射到路网中并进行压缩存储。基于分段的轨迹表征可以表达一定的轨迹语义信息，例如基于“行走”，“非行走”的分割方法可以反映出轨迹的运动状态的变化，基于路网匹配的轨迹表征方法可以反映轨迹在真实路段上的行走状态，但是它们难以表达轨迹点序列的时间信息，对轨迹整体语义的表达也是很有限的。

    \item \textbf{~~基于特征的表征：}其希望通过提取轨迹中的某些特征来表征轨迹，例如轨迹的速度、方向、角度、形状，或者轨迹的周期性\citeup{annoni2012analysis}等。其中Annoni等人\citeup{annoni2012analysis}将轨迹从原始空间转换到了谱空间来进行表示，将二维的轨迹转换为了一维的表示。Gariel等人\citeup{gariel2011trajectory}用不同的采样率对轨迹进行重新采样，这样就得到了长度相同的轨迹，之后再用主成分分析（PCA）来对重采样后的轨迹进行进一步压缩。还有一些方法利用了目前热门的神经网络，将轨迹喂给LSTM网络来学习轨迹的隐藏特征\citeup{ardakani2017encoding,gao2017identifying}。这类方法的不足之处在于它大多只能提取到轨迹的地理特征或者几何特征等简单的特征，对轨迹语义的表达能力依旧是有限的。
\end{itemize}

值得一提的是，上面提到的主流轨迹表征的关键点主要集中在地理信息上，几乎没有考虑具体的语义知识。因此，在轨迹表征上建立的索引系统会受到多种类型的检索任务的限制，从而产生了大量孤立的研究。例如，没有为多角度查询检索而设计的系统。给定特定的查询需求，例如：（1）检索城市中与给定犯罪区域最为相似的某些可疑区域。（2）通过给予犯罪分子的典型行动路线，从轨迹数据库中找出最可疑的运动轨迹。对于这种复杂的检索任务，用户必须手动将问题分解为单独的部分，然后分别构造查询。下文将描述主流的轨迹相似性度量及检索方法：





\section{轨迹相似度度量与轨迹检索}

\subsection{轨迹的相似性度量}
对轨迹进行表征，一项最基本的概念是定义了轨迹上的相似性度量。目前轨迹上相似性度量的算法很多，如：DTW距离，它允许一些点重复多次以获得最佳对齐\citeup{DTW,shokoohi2017generalizing}。还有最长公共子序列（LCSS）距离\citeup{LCSS}和实际序列编辑距离（EDR）距离\citeup{EDR}，他们的原理是消除噪声点引起的影响。Fr\'echet距离是另一种新颖的曲线之间的相似性度量，其考虑了沿着曲线\citeup{Frechet}的点的位置和顺序。在这里，本文简要介绍最具有代表性的DTW距离度量方式。

动态时间规整（Dynamic Time Warping）\citeup{DTW,senin2008dynamic}，简称为DTW，是一种非常经典的比较时间序列的相似性（距离）的度量方式。它的基本想法是为两个时间序列找到“最好的”匹配，为了实现这一点，时间序列在时间轴上被拉长或压缩（”Warping”）\citeup{salvador2007toward}，之后两个时间序列上的点再“最合适地”相互匹配。

假设两个时间序列$X = \left( x _ { 1 } , x _ { 2 } , \ldots , x _ { m } \right)$和$Y = \left( y _ { 1 } , y _ { 2 } , \ldots , y _ { n } \right)$的长度分别为$m$和$n$，时间序列规整的目标是找到$X$和$Y$的最佳的匹配路径$W$，记$W$为
\begin{equation}
W = \left( w _ { 1 } , w _ { 2 } , \ldots , w _ { K } \right).
\end{equation}
$K$为匹配路径$W$的长度，$K$满足条件$\max ( m , n ) < K < m + n - 1$；匹配路径$W$中的每个元素$W _ { k } ( 0 \leq k \leq K )$表示$X$中的某个点$x_i$与$Y$中的某个点$y_j$ 匹配到了一起，记作$w _ { k } = ( i , j )$，$W$应满足以下约束:

\begin{enumerate}
    \item \textbf{~~边界条件：}$w_1 = ( 1,1 )$以及$w _ { K } = ( m , n )$, 即$X$的第一个点和$Y$的第一个点要匹配到一起，$X$的最后一个点和$Y$的最后一个点要匹配到一起。
    \item \textbf{~~匹配顺序：}对于$W$中的任意两个点$w _ { k } = ( i , j )$和$w _ { k + 1 } = \left( i ^ { \prime } , j ^ { \prime } \right)$，要求$i ^ { \prime } \geq i$并且$y ^ { \prime } \geq y$。实际上，这个约束限制了$X$和$Y$只能线性地(在时间轴上拉长或缩短)匹配。
    \item \textbf{~~连续性：}对于$W$中的任意两个点$w _ { k } = ( i , j )$和$w _ { k + 1 } = \left( i ^ { \prime } , j ^ { \prime } \right)$，要求$i ^ { \prime } \geq i$，要求$i ^ { \prime } - i \leq 1$并且$y ^ { \prime } - y \leq 1$，这个约束要求$X$和$Y$中的每个点都有匹配。
\end{enumerate}

\pic[!htb]{基于DTW的轨迹相似度示意图\citeup{chen2013dynamic}}{width=150mm}{DTW}

满足以上约束的匹配有许多可能，动态时间规整是要找出其“最好”的匹配， 也就是使$X$和$Y$距离最小的匹配:
\begin{equation}
\text{dtw}( X , Y ) = \min \left( \sum _ { k = 1 } ^ { K } d \left( w _ { k } \right) \right)
\end{equation}
其中$d(\cdot)$是距离函数，例如$d \left( w _ { k } \right) = d \left( x _ { i } , y _ { j } \right) = \left| x _ { i } - y _ { j } \right|$。在求解两个时间序列的 DTW距离时，动态规划技术尝尝被用来加快寻找最佳匹配的速度。图\ref{DTW}展示了DTW的原理示意图。


\subsection{轨迹检索}
通常，轨迹的查询是启发式的：一般会查询感兴趣的重要节点（POI）或与指定的POI或轨迹最相似的轨迹。最关键的任务是定义相似性度量。几乎所有主流指标都只关注地理相似性。早期研究人员\citeup{agrawal1993efficient}使用两条轨迹所有采样点的总和距离，这要求轨迹具有统一长度，这在现实世界数据中是不现实的。动态时间扭曲（DTW）距离的提出就可以克服这一缺陷。

在轨迹的表示与各种度量的基础上，有研究者提出了许多轨迹索引结构。例如，STR-tree\citeup{pfoser2000novel}，TB-tree\citeup{pfoser2000novel}和HR-tree\citeup{nascimento1998towards}这三种树结构都概括了R-tree这种将空间和时间维度存储在一起的有效的空间数据库。之后，Chakka等人提出SETI\citeup{chakka2003indexing}来区分时空信息与空间索引系统，以提高检索效率。

然而，几乎所有传统的轨迹表示模型和索引系统都建立在轨迹数据的地理和时间特征上，因此语义检索和挖掘任务难以得到执行。


\section{词向量表征算法：word2vec模型}
在自然语言处理（NLP）任务中，寻找一种将自然语言数学化的表示方法对算法实现非常重要。最直观的方式，是将词汇表征为词向量的形式。下面介绍几种词向量形式：

首先是一种最为直观的独热向量（one-hot vector）表征方法\citeup{turian2010word}，其原理是用词语的编号位置来表示这个词语。具体来说，独热向量的长度即为词语集合的大小，整个向量只有目标词汇对应的位置处为1，其他位置均是0。这种表征方式很直观，但缺点也很直观：在大词汇量的数据库中及其稀疏，遭遇上了维度诅咒。此外，词汇间的相似度也难以通过这种方式来进行衡量。

而另一种最为普及的表征方式是分布式向量（distributed vector），也就是本工作使用的向量表示方法。其由Hinton\citeup{hinton1984distributed}提出，基本思想为：通过学习训练，将每一个词汇映射成一个固定长度的向量，其维度远远小于独热向量。在该向量空间中定义距离，就可以直接计算两个词语的语义距离。

在分布式向量表达的方法中，最著名的就是word2vec\citeup{mikolov2013distributed}模型。word2vec模型通过在给定语料上进行训练，最终可以实现将词典中的每一个单词映射到同一个向量空间中，定义向量之间的余弦相似性作为向量之间的距离，训练结束后可使得在语义上相似的单词对应的向量距离较小，而不相似的单词之间对应的向量的距离较大。例如“王后”和“国王”的距离比较小，“男孩”与“男人”对应的向量之间的距离较近，但是“北京”和“女孩”对应的向量之间的距离较远。

\pic[!htb]{word2vec学习出的词向量的二维展示图\citeup{mikolov2013distributed}}{width=150mm}{word2vec}
如图\ref{word2vec}是对word2vec模型学习结果的直观展示。学习到的高维词向量在通过主成分分析等方法降维到二维后，从可视化结果可见“国家”类别的单词之间距离较近，而“首都”类型的单词之间距离较近，但是两种类型的单词之间的距离较远。

除此之外，word2vec模型学习到的向量还满足“线性可加性”。即如果按照余弦相似性在全语料中搜索，可以发现，与$v$(中国)-$v$(北京)+$v$(巴黎)距离最近的向量对应的单词是“法国”。

另外，还有大量将一个词语，句子或者段落嵌入为隐向量的工作，其不仅将所有的单词映射到一个低维空间(低维是相对于词典的大小而言的)中，而且将每一个段落映射到同一个低维空间中，实现单词和段落的同步表征，最终实现相似的段落之间的相似性(余弦相似性较大，不相似的段落之间的相似性较小。其主要方法有用其组成单词的嵌入向量的加权平均\citeup{mitchell2010composition,iyyer2015deep},也有研究用更复杂的策略，例如迭代神经网络（recursive neural networks）\citeup{socher2011dynamic}、卷积神经网络（convolutional neural networks）\citeup{kalchbrenner2014convolutional}、循环神经网络等（recurrent neural networks）\citeup{tai2015improved}。

\subsection{CBOW和Skip-gram模型}

word2vec这种方法下有两种具体的模型:CBOW（Continous Bag of Words）模型和Skip-gram模型。两者的不同之处在于：CBOW用目标单词的上下文单词来表征其自己，而Skip-gram模型则用一个目标单词来表征它的上下文单词。下面将详细介绍这两个模型。
\pic[!htb]{CBOW模型和Skip-gram模型\citeup{mikolov2013distributed}}{width=120mm}{CBOW}

从图\ref{CBOW}中可以看到两个模型都包含三层：输入层，投影层和输出层。CBOW模型是通过一个单词$w_i$的上下文$context(w_i)$，即$\left\{ w _ { i - c } , w _ { i + 1 - c } \cdots w _ { i - 1 } , w _ { i + 1 } \cdots w _ { i + c }\right\}$来对$w_i$进行表征，其中$c$是选取的单词$w_i$的上下文的窗口大小。而skip-gram模型是用目标单词$w_i$来表征它的上下文单词$context(w_i)$。基于CBOW模型的word2vec模型的目标函数是：

\begin{equation}
f ( w ) = \prod _ { w \in C } p ( w | \operatorname { context } ( w ) ) )
\end{equation}

即在已知每一个单词$w$的上下文$context(w)$出现的情况下，该单词出现的概率最大。整个词库出现的条件概率就是词库中的每一个单词出现的条件概率连乘的结果。因为在取对数之后函数单调性不变，对数变换之后基于CBOW模型的word2vec模型的目标函数就变成:

\begin{equation}
L = \sum _ { w \in C } \log \left( p \left( w | \operatorname { context } \left( w \right) \right) \right). 
\end{equation}

同理，基于Skip-gram模型的目标函数为:
\begin{equation}
L = \sum _ { w \in C } \log \left( p \left( \operatorname { context } \left( w \right) | w \right) \right)
\end{equation}

即在知已知每一个单词出现的情况下，整个词库出现的条件概率值等于词库中的每一个单词出现的条件概率连乘，该概率就是基于Skip-gram的word2vec模型在训练中要优化的目标函数。

\subsection{基于Hierarchical Softmax模型的词向量学习方法}

\pic[!htb]{基于CBOW的Hierarchical Softmax的模型示意图}{width=70mm}{softmax}
模型概述：如图\ref{softmax}所示，模型分成了三层，即输入层，投影层和输出层。
\begin{itemize}
    \item \textbf{~~输入层：}CBOW模型的输入层包含某一个单词$w_i$的上下文$2c$个单词的词向量$v \left( \text {context} ( w ) _ { 1 } \right) , v \left( \text {context} ( w ) _ { 2 } \right) , \ldots , v \left( \text {context} ( w ) _ { 2 c } \right) \in \mathbb { R } ^ { m }$，skip-gram模型在输入层的输入就是一个单词的向量$v(w)$。这里$m$是词向量的维度（长度）。
    \item \textbf{~~投影层：}CBOW模型的投影层就是将输入层的$2c$个向量求和累加得到$X _ { w } = \sum _ { i = 1 } ^ { 2 c } \operatorname { context } ( w ) _ { i } ) \in \mathbb { R } ^ { m }$，而skip-gram模型的投影层为简单的恒等投影，将 $v(w)$投影到$v(\text{context}(w))$（这种多余的操作是为了和CBOW模型进行对比）。
    \item \textbf{~~输出层：}为Huffman树，$N$个叶子结点$(N=|\mathcal{D}|)$分别对应于字典$\mathcal{D}$中的$N$个词，其构造根据词频来进行。非叶子节点共有$N-1$个，对应于图\ref{softmax}中黄色的点，树中叶子结点中的数字代表词频。
\end{itemize}

输出层的计算方法介绍在前面提到的Huffman树的输出层中，某一个单词出现的条件概率的计算方法。在此之前，有必要引入相关的符号。对于Huffman树中的某个叶子结点，假设它对应于字典$\mathcal{D}$中的词$w$，记

\begin {itemize}
{
\item[] 1. ${path}^w$ ：从根节点为起点，$w$对应叶结点为终点的路径。

\item[] 2. $l^w$ ：路径${path}^w$中结点数目。

\item[] 3. ${path}_{1}^{w}$, ${path}_{2}^{w}$, $\cdots$, ${path}_{l^w}^{w}$ ：路径 ${path}^w$中的$l^w$个结点，其中${path}_{1}^{w}$表示根节点，${path}_{l^w}^{w}$对应词$w$对应的叶子结点。

\item[] 4. $d_{2}^{w}$, $d_{3}^{w}$, $\cdots$, $d_{l^w}^{w}\in \{0,1\}$ : 词$w$的Huffman编码，长度为$l^w - 1$，$d_{j}^{w}$表示路径${path}^w$中第$j$个结点对应的Huffman编码。

\item[] 5. $\theta_{1}^{w}$, $\theta_{2}^{w}$, $\cdots$, $\theta_{l^w - 1}^{w}\in \mathbb{R}^{m}$ : 路径${path}^w$中非叶子结点对应的向量，其中$\theta_{j}^{m}$表示路径 ${path}^w$中第$j$个非叶子结点的向量。
}
\end {itemize}

引入上面的符号之后，下面本文介绍输出层的概率计算方法。

对CBOW模型来说，输出层的输出结果是在已知单词$w$的上下文$\operatorname {context}(w)$出现的条件下，单词$w$出现的条件概率。对于Skip-gram模型，输出层结果的含义是，在已知单词$w$出现的条件下，它的上下文$\operatorname {context}(w)$出现的条件概率。此处首先介绍CBOW模型下的条件概率计算方法。

将Huffman树视为一个多层分类器，树中的每一个非叶子结点看做一个二分类器，指示分类结果应该属于左子树还是右子树。
假如词典中的所有单词的集合是$\{a,cat,is,climbing,the,tree\}$,按照它们在词典中出现的频率构建一棵如图\ref{softmax}所示的Huffman树，从根结点开始，左分支标记为1表示负类，右分支标记为0表示正类（可以根据个人习惯进行定义，下文都按照这种分类方法就进行阐释）。这样词典中的每一个单词在Huffman树中都有从根结点出发到对应叶子结点结束的唯一路径，也对应唯一一串指示分类结果的编码。以单词“\emph{climbing}”为例，从根节点出发到达“\emph{climbing}”这个叶子结点会经历四次二分类，分类编码是$\{d_{2}^{w}, d_{3}^{w},d_{4}^{w} ,d_{5}^{w}\} = \{1,0,0,1\}$ 。

根据逻辑回归\citeup{harrell2001ordinal}的定义，一个结点被分为正类的概率是
\begin{equation}
P^+ =\sigma(x_{w}^{T}\theta )
\end{equation}
被分为负类的概率显然就是
\begin{equation}
 P^- =1 - \sigma(x_{w}^{T}\theta )
\end{equation}
其中$x$表示待分类变量的向量，$\theta$表示用于分类的参数向量，$\sigma(x) = 1/(1+e^{-x^{T}} )$\
是取值范围在${0,1}$之间的sigmoid函数。对应到Huffman树中，$x$应该对应投影层的输出向量$X_w$，$\theta$应该对应Huffman树中的非叶子节点$\theta_{j}^{w}$。

从根节点出发到达“climbing”这个叶子结点经历的四次二分类的概率结果相乘即可得到输出层的条件概率$P(\text{climbing}|\text{context(climbing)})$的值：

% 1. 第一次：$P(d_{2}^{w}|X_w, \theta_{1}^{w}) = 1- \sigma(x_{w}^{T}\theta_{1}^{w})$

% 2. 第二次：$P(d_{3}^{w}|X_w, \theta_{2}^{w}) = \sigma(x_{w}^{T}\theta_{2}^{w})$

% 3. 第三次：$P(d_{4}^{w}|X_w, \theta_{3}^{w}) = \sigma(x_{w}^{T}\theta_{3}^{w})$

% 4. 第四次：$P(d_{5}^{w}|X_w, \theta_{4}^{w}) = 1- \sigma(x_{w}^{T}\theta_{4}^{w})$


\begin{equation}
P(\text{climbing}|\text{context(climbing)}) = \prod_{j=2}^{5}P(d_{j}^{w}|x_w,\theta_{j-1}^{w})
\end{equation}

% 也就是说，词典中的一个单词$w$在Huffman树输出层的输出结果，也就是在其上下文$context(w)$出现的情况下的条件概率值，就等于将$\operatorname {context}(w)$中的所有向量求和之后，在Huffman树中从根结点到$w$对应的叶子结点经历的每一次二分类结果的连乘。

条件概率$P(w|\operatorname{context}(w))$的一般公式可以写为
\begin{equation}
P(w|\operatorname{context}(w)) = \prod_{j=2}^{l^w}P(d_{j}^{w}|x_w,\theta_{j-1}^{w})
\label{def:conditional probability}
\end{equation}
其中，
\begin{equation}
P(d_{j}^{w}|x_w,\theta_{j-1}^{w}) =\left\{\begin{matrix}

\sigma(X_w\theta_{j-1}^{w}) & d_{j}^{w} = 0

\\
1-\sigma(X_w\theta_{j-1}^{w}) & d_{j}^{w} = 1
\end{matrix}\right.
\end{equation}
写成整体表达式就是：
\begin{equation}
P(d_{j}^{w}|x_w,\theta_{j-1}^{w}) = [\sigma(X_{w}^{T}\theta_{j-1}^{w})]^{1-d_{j}^{w}}\cdot [1-\sigma(X_{w}^{T}\theta_{j-1}^{w})]^{d_{j}^{w}}
\end{equation}
将公式\ref{def:conditional probability}代入对数似然函数\ref{def:CBOW objective function}，可得基于CBOW模型和Hierarchical Softmax框架的
word2vec模型的目标函数为：
\begin{equation}
\begin{aligned}
 \mathcal{L} &= \sum_{w\in C}\prod_{j=2}^{l^{w}} \{|\sigma(X_{w}^{T}\theta_{j-1}^{w})|^{1-d_{j}^{w}} \cdot |1-\sigma(X_{w}^{T}\theta_{j-1}^{w})|^{d_{j}^{w}}\}  \\
&=\sum_{w \in C}\sum_{j=2}^{l^{w}}\{(1-d_{j}^{w})\cdot log[\sigma(X_{w}^{T}\theta_{j-1}^{w})]  +  d_{j}^{w}\cdot log[1-\sigma(X_{w}^{T}\theta_{j-1}^{w})]\} 
\end{aligned}
\end{equation}

基于Skip-gram模型的Hierarchical Softmax模型同理可推，具体目标函数可以参考原文\citeup{mikolov2013distributed}。

\subsection{基于Nagetive sampling模型的词向量学习方法}
使用Nagetive sampling模型优化目标方程的总体思想是对训练集（字典）来说，将正样本队出现的概率最大化，而将负样本出现的概率进行最小化。

以CBOW模型为例，对于给定的正样本$(\operatorname{context}(w), w)$，除$w$之外的其他的单词就是负样本。对整个词典来说我们希望最大化
\begin{equation}
g(w) = \prod_{u \in \{w\}\cup NEG(w))} P(u|\operatorname{context}(w))
\label{def: nagetive sampling objective function}
\end{equation}
其中，
\begin{equation}
P(u|\operatorname{context}(w)) = [\sigma(x_{w}^{T}\theta^u) ]^{L^w(u)}\cdot [1-\sigma(x_{w}^{T})\theta^u]^{1-L^w(u)}
\label{def:nagetive sampling conditional probability}
\end{equation}
其中$L^w(u)$是一个指示函数，指示单词是否正样本：若单词为正样本则$L^w(u)$的值将置为1，否则为0。

将公式\ref{def:nagetive sampling conditional probability}带入公式\ref{def: nagetive sampling objective function}可得
\begin{equation}{}
g(w) = \sigma(x_{w}^{T}\theta^w)\prod_{u \in NEG(w))}[1- \sigma(x_{w}^{T}\theta^u)] 
\end{equation}

最大化$g(w)$等价于最大化正样本出现概率$\sigma(x_{w}^{T}\theta^w)$的同时，再将负样本出现的概率$\sigma(x_{w}^{T}\theta^u)$进行最小化。


\section{同步聚类算法}
由于本文的方法用了同步聚类算法，所以在这里提出来单独介绍。同步聚类算法（Synchronization-based Clustering，Sync）是根据物理、生物学、化学以及社会科学中提到的同步现象所提出的一种聚类算法。而同步现象最早是有Acebron等人提出来的\citeup{acebron2005kuramoto}。

\subsection{同步现象及应用}
试想，在漆黑的丛林中，各种昆虫开始为了生存而进行各项活动。到处都是蝉鸣，却找不到声源的具体来源。此时，如果有一只萤火虫开始若隐若现的闪光，那这闪光马上就不会孤单：一瞬间，练成片的火光反复一下子被这气氛点燃，开始热烈地一起闪耀。这闪光，是由成百上千的萤火虫一起发出的，但它们并不是各自成灯，而是整齐划一的遵循着某种规律一起变亮，一起变暗，反复一堆节日彩灯被看不到的电网练了起来而随着时强时弱的电流周期性的闪耀。这就是同步现象引申出的地方，之后，更多的研究者发现了同步现象存在的其他领域\citeup{seliger2002plasticity,network,sync,shaoBook,shao2015community,effective,shao2016scalable,shao2017robust,shao2017Cosync,kis}。

Kuramoto在1975年提出了Kuramoto模型来对同步现象进行了一个刻画，成为此类现象中最被认同的模型。之后Seliger等人\citeup{seliger2002plasticity}也对Kuramoto模型进行了讨论，并改进提出基于弹簧振子刻画的一般化Kuramoto模型。Arenas等人\citeup{network}将Kuramoto 模型推广到了网络分析，并研究了网络的拓扑结构和动态时间范围之间的关系。他们的研究对网络的拓扑结构、谱分析、同步动态机制之间的关系进行了联系。在生物学中，Kim等人\citeup{gene}也利用了同步模型来对细胞进行检测，并从中发现了成组的新基因。与之相似的，Shao等人\citeup{shao2017Cosync}也改进了同步算法并提出了一种双边聚类算法，使相似的蛋白质以及相似的基因能够能够同时得到发掘。近年来，更多基于同步的算法被提了出来\citeup{sync,shao2017robust,Shao2010,effective,shaoBook}，Kuramoto模型也在这过程中被一步一步被改进。


\subsection{同步聚类以及优势}
在同步算法Sync\citeup{sync}中，一种基于Kuramoto的数据挖掘聚类被提了出来，在这个工作中，一种基于局部聚类的动态机制是整个工作的核心，而最小描述长度（MDL）则用来决定动态交互的程度。整体来说，这种算法支持高质量的社区发现，其对噪声数据也更加敏感。

基于GPS的轨迹数据通常由一组点表示，由于不同的轨迹通常具有不同的长度和不同的采样率，这并不是一个直接挖掘轨迹数据的好方法。基于此，一种直观的改进方法是将这些点人为分组或者说聚类为具有语义信息的不同区域。但是，传统的聚类方法不适合这项任务。例如，k-means样式聚类算法需要人为给定聚类数$k$，这是一个依靠经验的步骤，并不能广泛适用于各种情况。其次，k-means算法得到的聚类不是均匀分布的，因此其表示错误不能被误差界阈值$\zeta$界定。与基于k-means的聚类算法相比，同步聚类的优势在于它可以自动生成具有有意义数目的聚类结果。具体而言，簇的数量受到交互范围$\epsilon$的影响：簇的数量将随着$\epsilon$的增加而减少。此外，确定交互范围比提供正确数量的聚类更直观。前者可以控制聚类过程，这促使我们在某种情况下使用$\epsilon$作为误差界限$\zeta$的指示符，即，当大多数点或者几乎所有点都具有小于$\zeta$的代表性误差时，可认为此时误差界阈值$\zeta$就是交互范围$\epsilon$。相反地，如果我们使用k-means作为轨迹压缩的聚类方法，则只能全局地执行聚类，这并不利于控制压缩过程。

聚类模型的另一个分支是基于DBSCAN\citeup{ester1996density}或OPTICS\citeup{ankerst1999optics}的搜索。通过寻找密集区域来聚类。然而，这种算法的结果中，聚类簇是任意形状的，这可能违反现实世界区域约束，并可能在我们的轨迹压缩场景下产生无意义的聚类簇。例如，城市中的一条主要道路可能被检测为单个聚类簇，这不适合轨迹压缩。相比之下，基于同步的Sync聚类方法倾向于生成均匀分布的聚类簇，这将使这些有意义的聚类簇成为地图上的压缩点。因此，这些聚类中心可以直观且合理地呈现轨迹压缩结果。


在这些工作的推动下，本文从动态角度观察轨迹压缩，并基于同步原理提取多分辨率轨迹数据抽象。本文是第一个将同步概念应用于数据压缩的工作。

\section{可解释的推荐系统}
可解释推荐系统指的是那些给出了问题原因个性化推荐算法，它们不仅能向用户提供建议，还提供解释以使用户或系统设计者能了解推荐此类商品的原因。算法通过这种方式，提高了推荐系统的有效性、运行效率、算法说服力以及用户满意度。
为了突出整个推荐系统研究中可解释推荐的位置，Zhang等人的综述\citeup{zhang2018explainable}将大多数现有的个性化推荐研究分类为广泛的几个概念。具体而言，许多推荐研究任务可以归类为解决五个问题：什么时候？在什么地方？什么人？推荐了什么？为什么？它们通常对应于：能感知时间的推荐（什么时候），基于位置的推荐（在什么地方），社交推荐（什么人），结合应用场景的推荐（推荐什么）和可解释的推荐（为什么）。而可解释的推荐系统主要旨在回答为什么的问题。

从人工智能研究中，从推荐过程以及推荐结果这两个角度来看，可解释的推荐一般考虑推荐的方法（即推荐过程）的可解释性。通过这种方式，可解释的推荐旨在设计以人类方式工作的可解释模型，并且这种模型通常还导致推荐结果（即推荐结果）的可解释性。目前大多数可解释的推荐研究都属于这一类，旨在了解该过程的工作原理，通常它们被称为基于模型的可解释推荐。可解释性推荐的另一种方法是我们只关注推荐结果。通过这种方式，本文将推荐模型视为一个复杂的黑盒子而忽略了它的可解释性，而是开发了单独的方法来解释这个黑盒子产生的推荐结果。属于此类别的方法通常称为后可解释推荐方法。

目前，可解释性推荐系统的方法可以分为以下几类：

\subsection{基于矩阵分解的可解释性推荐系统}
目前，很多可解释推荐模型都是基于矩阵分解的模型进行的，用户和商品的潜在特征被嵌入到低维度的隐空间中，每个维度都代表着用户决策的特定因素。然而，由于隐因子的每个维度的具体含义我们无法得知，整个推荐结果是难以解释的。Zhang等人\citeup{zhang2014explicit}提出了显式因子模型（EFM），其基本思想是向用户推荐在其关心的功能上表现良好的产品，如图\ref{EFM}所示。具体地，该方法从文本用户评论中提取显式的产品特征，并将矩阵分解中的每个潜在维度与特定显式特征对齐，使得分解/预测过程可以是可跟踪的，以提供对推荐结果的明确解释。该方法可以提供伴随着推荐结果的个性化解释，例如，“向您推荐该产品的原因是由于您对特定特征感兴趣，并且该产品在该特征上表现良好”。该模型甚至可以通过告诉用户”产品在您关注的功能上表现不佳”来提供不推荐该产品的原因，这有助于提高推荐系统的可信度。因为用户对项目特征的偏好是动态的，并且可能随着时间而变化，Zhang等人\citeup{zhang2015daily}之后又进一步扩展了该想法，通过了对用户在每天兴趣点的动态变化来动态地建模用户特征。

\pic[!htb]{EFM模型简介\citeup{zhang2014explicit}。(a)基本工作原理为推荐用户关注的特征上表现良好的产品。(b)每一个评论都会转换为用户对特征的态度。(c)抽取出用户-特征矩阵来表现每个用户的关注点，抽取出商品-特征矩阵来体现商品质量。(d)伴随着推荐结果的推荐解释信息。}{width=150mm}{EFM}

Chen等人\citeup{chen2016learning}进一步将EFM模型扩展到了张量分解模型上。区别于EFM模型，作者从文本评论中提取了产品特征，并构建了用户与商品的特征多维数据集。基于该多维数据集，文中用商品对来进行排名以预测用户对特征以及商品的偏好，并基于这些预测结果提供个性化推荐。该模型之后又扩展到同时考虑多类产品，这有助于缓解推荐系统中的数据稀疏性问题。

\subsection{基于话题建模的可解释性推荐系统}
如今很多信息都反映在用户的购买评论里，收集出每个用户的评论数据，里面可以抽取提取出该用户的性格、购买能力以及爱好等特征。在Mcauley等人\citeup{mcauley2013hidden}，Wu等人\citeup{wu2015flame}以及Zhao等人\citeup{zhao2015sar}的工作中，用户的决策以及特征描绘了都用了用户的评论词云信息。其中Mcauley等人\citeup{mcauley2013hidden}提出了用隐藏的从文本评论中抽取出的主题来对隐因子进行解释的算法。他们提出了Hidden Factor and Topic (HFT)模型，巧妙地用softmax函数将隐因子模型和Latent Dirichlet Allocation (LDA)中每一维度联系了起来。这种做法不仅提升了模型预测的准确度，还通过将每个用户的潜藏因子隐射到LDA模型中学习到的主题上来帮助用户来理解其决策以及推荐的原因。

遵循着这个想法，Tan等人\citeup{tan2016rating}提出了基于评论信息在统一语义空间中对商品属性和用户偏好进行建模的方法。在建模过程中，商品被嵌入成一种主题推荐性分布，高分商品的主题还将进一步被增强重要性。类似地，用户也将被嵌入在相同的空间中，具体的分布将由他/她的历史评分行为确定。最后，可推荐性和偏好分布将被整合到潜在因子分解框架中来拟合现实数据集，并且基于所学习的潜在主题推导出对推荐商品的解释。

\subsection{基于知识图谱的可解释性推荐系统}
知识图谱包含用户和商品的丰富信息，有助于为商品推荐生成直观且更加定制的解释。最近，研究者们开始利用知识图谱来解释推荐系统。Catherine等人\citeup{catherine2017explainable}阐述了如何通过知识图谱来利用外部知识来产生解释。该方法使用个性化的PageRank规划，联合对商品和知识图谱实体进行排序，以产生推荐及其解释，并应用到了电影推荐方案中。

Ai等人\citeup{ai2018learning}提出的方法与上述方法不同，其构造了一个包含用户、商品以及其他实体信息的知识图谱，刻画了用户对商品的购买、评价，以及商品类别信息的归属等多方信息，对于推荐的解释将从这个知识图谱上用户到商品的最短路径推理得出。

Wang等人\citeup{wang2018ripple}提出了水波网络（Ripple Network)来克服基于嵌入与基于路径的知识图谱的方法不足，其主要思想是类似于水波在水面上的传播过程，来刻画用户的潜在爱好在知识图谱中的扩散过程。而且每个用户的综合爱好将引发并扩散出多个水波，这些水波将拨动知识图谱上相应位置处的商品，以一定概率触发推荐，并给出生动形象的解释。

\section{本章小结}
本章介绍了目前国际主流的轨迹压缩以及轨迹表征的方式。一些算法的基本思想被单独提出来帮助读者理解本文的主要概念。其次，自然语言处理中的词汇嵌入表征的方法word2vec也被详述，这对后文轨迹表征的理解非常关键。同时，在本文中轨迹压缩与轨迹表征所用到的同步聚类技术也被介绍，并与传统的k-means，DBSCAN等聚类方式做了一个比较，说明了其在轨迹数据集中的独特优势。最后，本章介绍了目前主流的可解释性推荐系统的方法分类，以及主要缺陷。

% \newpage\mbox{}\thispagestyle{empty}\newpage
